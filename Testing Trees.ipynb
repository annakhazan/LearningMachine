{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import feature_eng_function as f_eng\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from PCA_function import pca_data100\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest_train = pd.read_csv(\"data/train.csv\")\n",
    "forest_test = pd.read_csv(\"data/test.csv\")\n",
    "forest_base_train = pd.read_csv(\"data/train_eng.csv\")\n",
    "forest_base_test = pd.read_csv(\"data/test_eng.csv\")\n",
    "forest_100_train = pd.read_csv(\"data/train_100.csv\")\n",
    "forest_100_test = pd.read_csv(\"data/test_100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create arrays for each of the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = forest_train['Cover_Type']\n",
    "ID = forest_test['Id']\n",
    "\n",
    "X_train = forest_train[[col for col in forest_train.columns.tolist() if col not in ['Id','Cover_Type']]].values\n",
    "X_train_base = forest_base_train[[col for col in forest_base_train.columns.tolist() if col not in ['Id','Cover_Type']]].values\n",
    "X_train_100 = forest_100_train[[col for col in forest_100_train.columns.tolist() if col not in ['Id','Cover_Type']]].values\n",
    "\n",
    "X_test = forest_test[[col for col in forest_test.columns.tolist() if col not in ['Id','Cover_Type']]].values\n",
    "X_test_base = forest_base_test[[col for col in forest_base_test.columns.tolist() if col not in ['Id','Cover_Type']]].values\n",
    "X_test_100 = forest_100_test[[col for col in forest_100_test.columns.tolist() if col not in ['Id','Cover_Type']]].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Models\n",
    "optimized_100_cv5_GBM = pickle.load( open('pickles/optimized_100_cv5_GBM.p', 'rb') )\n",
    "optimized_100_cv10_GBM = pickle.load( open('pickles/optimized_100_cv10_GBM.p', 'rb') )\n",
    "optimized_100_default_GBM = pickle.load( open('pickles/optimized_100_default_GBM.p', 'rb') )\n",
    "optimized_base_cv5_GBM = pickle.load( open('pickles/optimized_base_cv5_GBM.p', 'rb') )\n",
    "optimized_base_cv10_GBM  = pickle.load( open('pickles/optimized_base_cv10_GBM.p', 'rb') )\n",
    "optimized_base_default_GBM  = pickle.load( open('pickles/optimized_base_default_GBM.p', 'rb') )\n",
    "optimized_kaggle_default_GBM = pickle.load( open('pickles/optimized_kaggle_default_GBM.p', 'rb') )\n",
    "optimized_kaggle_cv5_GBM = pickle.load( open('pickles/optimized_kaggle_cv5_GBM.p', 'rb') )\n",
    "optimized_kaggle_cv10_GBM = pickle.load( open('pickles/optimized_kaggle_cv10_GBM.p', 'rb') )\n",
    "\n",
    "# Random Forest Models\n",
    "optimized_100_cv5_RF = pickle.load( open('pickles/optimized_100_cv5_RF.p', 'rb') )\n",
    "optimized_100_cv10_RF = pickle.load( open('pickles/optimized_100_cv10_RF.p', 'rb') )\n",
    "optimized_base_cv5_RF = pickle.load( open('pickles/optimized_base_cv5_RF.p', 'rb') )\n",
    "optimized_base_cv10_RF = pickle.load( open('pickles/optimized_base_cv10_RF.p', 'rb') )\n",
    "optimized_kaggle_cv5_RF = pickle.load( open('pickles/optimized_kaggle_cv5_RF.p', 'rb') )\n",
    "optimized_kaggle_cv10_RF = pickle.load( open('pickles/optimized_kaggle_cv10_RF.p', 'rb') )\n",
    "\n",
    "#Extra Trees Models\n",
    "optimized_kaggle_cv10_ET = pickle.load( open('pickles/optimized_kaggle_cv10_ET.p', 'rb') )\n",
    "optimized_base_cv10_ET = pickle.load( open('pickles/optimized_base_cv10_ET.p', 'rb') )\n",
    "optimized_100_cv10_ET = pickle.load( open('pickles/optimized_100_cv10_ET.p', 'rb') )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lists of models to test on for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_100 = [\n",
    "    ('optimized_100_cv5_GBM',optimized_100_cv5_GBM),\n",
    "    ('optimized_100_cv10_GBM',optimized_100_cv10_GBM),\n",
    "    ('optimized_100_default_GBM',optimized_100_default_GBM),\n",
    "    ('optimized_100_cv5_RF',optimized_100_cv5_RF),\n",
    "    ('optimized_100_cv10_RF',optimized_100_cv10_RF),\n",
    "    ('optimized_100_cv10_ET',optimized_100_cv10_ET)\n",
    "]\n",
    "\n",
    "models_base = [\n",
    "    ('optimized_base_cv5_GBM',optimized_base_cv5_GBM),\n",
    "    ('optimized_base_cv10_GBM',optimized_base_cv10_GBM),\n",
    "    ('optimized_base_default_GBM',optimized_base_default_GBM),\n",
    "    ('optimized_base_cv5_RF',optimized_base_cv5_RF),\n",
    "    ('optimized_base_cv10_RF',optimized_base_cv10_RF),\n",
    "    ('optimized_base_cv10_ET',optimized_base_cv10_ET)\n",
    "]\n",
    "\n",
    "models_kaggle = [\n",
    "    ('optimized_kaggle_cv5_GBM',optimized_kaggle_cv5_GBM),\n",
    "    ('optimized_kaggle_cv10_GBM',optimized_kaggle_cv10_GBM),\n",
    "    ('optimized_kaggle_default_GBM',optimized_kaggle_default_GBM),\n",
    "    ('optimized_kaggle_cv5_RF',optimized_kaggle_cv5_RF),\n",
    "    ('optimized_kaggle_cv10_RF',optimized_kaggle_cv10_RF),\n",
    "    ('optimized_kaggle_cv10_ET',optimized_kaggle_cv10_ET) \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test each of the models and import into data frame to view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_kaggle_cv5_GBM best score:  0.778637566138\n",
      "{'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 1} scoring by:  make_scorer(accuracy_score) \n",
      "\n",
      "optimized_kaggle_cv10_GBM best score:  0.777777777778\n",
      "{'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 1} scoring by:  make_scorer(accuracy_score) \n",
      "\n",
      "optimized_kaggle_cv5_RF best score:  0.654365079365\n",
      "{'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 700} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n",
      "optimized_kaggle_cv10_RF best score:  0.666997354497\n",
      "{'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 300} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n",
      "optimized_kaggle_cv10_ET best score:  0.785119047619\n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 1000} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models_kaggle)):\n",
    "    try:\n",
    "        print (models_kaggle[i][0], \"best score: \", models_kaggle[i][1].best_score_)\n",
    "        print (models_kaggle[i][1].best_params_, \"scoring by: \", models_kaggle[i][1].scorer_, \"\\n\")\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_base_cv5_GBM best score:  0.780952380952\n",
      "{'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 1} scoring by:  make_scorer(accuracy_score) \n",
      "\n",
      "optimized_base_cv10_GBM best score:  0.778439153439\n",
      "{'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 1} scoring by:  make_scorer(accuracy_score) \n",
      "\n",
      "optimized_base_cv5_RF best score:  0.650925925926\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 100} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n",
      "optimized_base_cv10_RF best score:  0.662764550265\n",
      "{'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 200} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n",
      "optimized_base_cv10_ET best score:  0.778174603175\n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 700} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models_base)):\n",
    "    try:\n",
    "        print (models_base[i][0], \"best score: \", models_base[i][1].best_score_)\n",
    "        print (models_base[i][1].best_params_, \"scoring by: \", models_base[i][1].scorer_, \"\\n\")\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_100_cv5_GBM best score:  0.797883597884\n",
      "{'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 1} scoring by:  make_scorer(accuracy_score) \n",
      "\n",
      "optimized_100_cv10_GBM best score:  0.797354497354\n",
      "{'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 1} scoring by:  make_scorer(accuracy_score) \n",
      "\n",
      "optimized_100_cv5_RF best score:  0.66541005291\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 1000} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n",
      "optimized_100_cv10_RF best score:  0.671362433862\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 1000} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n",
      "optimized_100_cv10_ET best score:  0.811706349206\n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 1000} scoring by:  <function _passthrough_scorer at 0x114bc30d0> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models_100)):\n",
    "    try:\n",
    "        print (models_100[i][0], \"best score: \", models_100[i][1].best_score_)\n",
    "        print (models_100[i][1].best_params_, \"scoring by: \", models_100[i][1].scorer_, \"\\n\")\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_kaggle_cv5_GBM predictions complete!\n",
      "optimized_kaggle_cv10_GBM predictions complete!\n",
      "optimized_kaggle_default_GBM predictions complete!\n",
      "optimized_kaggle_cv5_RF predictions complete!\n",
      "optimized_kaggle_cv10_RF predictions complete!\n",
      "optimized_kaggle_cv10_ET predictions complete!\n",
      "optimized_base_cv5_GBM predictions complete!\n",
      "optimized_base_cv10_GBM predictions complete!\n",
      "optimized_base_default_GBM predictions complete!\n",
      "optimized_base_cv5_RF predictions complete!\n",
      "optimized_base_cv10_RF predictions complete!\n",
      "optimized_base_cv10_ET predictions complete!\n",
      "optimized_100_cv5_GBM predictions complete!\n",
      "optimized_100_cv10_GBM predictions complete!\n",
      "optimized_100_default_GBM predictions complete!\n",
      "optimized_100_cv5_RF predictions complete!\n",
      "optimized_100_cv10_RF predictions complete!\n",
      "optimized_100_cv10_ET predictions complete!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models_kaggle)):\n",
    "    m = models_kaggle[i][0]\n",
    "    mod = models_kaggle[i][1]\n",
    "    filename = str('submissions/' + m + '.csv')\n",
    "    if os.path.isfile(filename):\n",
    "        pass\n",
    "    else:\n",
    "        results = pd.DataFrame(ID)\n",
    "        results['Cover_Type'] = mod.predict(X_test)\n",
    "        results['Cover_Type'] = results['Cover_Type'].astype(int)\n",
    "        results.to_csv(filename, index=False)\n",
    "    print (m, \"predictions complete!\")\n",
    "    \n",
    "for i in range(len(models_base)):\n",
    "    m = models_base[i][0]\n",
    "    mod = models_base[i][1]\n",
    "    filename = str('submissions/' + m + '.csv')\n",
    "    if os.path.isfile(filename):\n",
    "        pass\n",
    "    else:\n",
    "        results = pd.DataFrame(ID)\n",
    "        results['Cover_Type'] = mod.predict(X_test_base)\n",
    "        results['Cover_Type'] = results['Cover_Type'].astype(int)\n",
    "        results.to_csv(filename, index=False)\n",
    "    print (m, \"predictions complete!\")\n",
    "    \n",
    "for i in range(len(models_100)):\n",
    "    m = models_100[i][0]\n",
    "    mod = models_100[i][1]\n",
    "    filename = str('submissions/' + m + '.csv')\n",
    "    if os.path.isfile(filename):\n",
    "        pass\n",
    "    else:\n",
    "        results = pd.DataFrame(ID)\n",
    "        results['Cover_Type'] = mod.predict(X_test_100)\n",
    "        results['Cover_Type'] = results['Cover_Type'].astype(int)\n",
    "        results.to_csv(filename, index=False)\n",
    "    print (m, \"predictions complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
