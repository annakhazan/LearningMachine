{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline Model Testing\n",
    "\n",
    "Data source: https://www.kaggle.com/c/forest-cover-type-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annakhazan/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from feature_eng_function import feature_eng_forest, forest_interactions\n",
    "from confusion_matrix_score_function import confusion_matrix_scoring \n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 56)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_train = pd.read_csv('../ml_project_data/train.csv')\n",
    "original_cols = list(forest_train.columns)\n",
    "original_cols.remove('Cover_Type')\n",
    "forest_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565892, 55)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_test = pd.read_csv('../ml_project_data/test.csv')\n",
    "output_df = forest_test[['Id']]\n",
    "forest_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 97)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_train_eng = pd.read_csv('../ml_project_data/train_eng.csv')\n",
    "forest_train_eng_cols = list(forest_train_eng.columns)\n",
    "forest_train_eng_cols.remove('Cover_Type')\n",
    "forest_train_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565892, 96)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_test_eng = pd.read_csv('../ml_project_data/test_eng.csv')\n",
    "forest_test_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# forest_train_interactions = pd.read_csv('../ml_project_data/train_interactions.csv')\n",
    "# all_cols = list(forest_train_interactions.columns)\n",
    "# all_cols.remove('Cover_Type')\n",
    "# forest_train_interactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forest_test_interactions = pd.read_csv('../ml_project_data/test_interactions.csv')\n",
    "# forest_test_interactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 102)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_train_100 = pd.read_csv('../ml_project_data/train_100.csv')\n",
    "top_100_cols = list(forest_train_100.columns)\n",
    "top_100_cols.remove('Cover_Type')\n",
    "forest_train_100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565892, 101)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_test_100 = pd.read_csv('../ml_project_data/test_100.csv')\n",
    "forest_test_100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the continuous features\n",
    "###### We will try Normalization, Standardized Scaling, and MinMax Scaling\n",
    "###### Note: there is no need to impute any data points as this is a pretty clean data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk_size = 0.1 #Validation chunk size\n",
    "seed = 0 # Use the same random seed to ensure consistent validation chunk usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transformTrainData(data):\n",
    "    #Reorder the data to have continuous variables come first\n",
    "    continuous = []\n",
    "    categorical = []\n",
    "    final_columns = []\n",
    "    for col in data.columns.tolist():\n",
    "        if col == 'Cover_Type':\n",
    "            pass\n",
    "        elif data[col].nunique() > 4:\n",
    "            continuous.append(col)\n",
    "        else:\n",
    "            categorical.append(col)\n",
    "    final_columns.extend(continuous)\n",
    "    final_columns.extend(categorical)\n",
    "    final_columns.append('Cover_Type')\n",
    "    data = data[final_columns]\n",
    "    num_row, num_cols = data.shape\n",
    "    cols = data.columns\n",
    "    size = len(continuous) # Number of continuous columns\n",
    "    #Create the data arrays for model building\n",
    "    val_array = data.values\n",
    "    X = val_array[:,0:(num_cols-1)]\n",
    "    y = val_array[:,(num_cols-1)]\n",
    "    X_train, X_val, y_train, y_val = cross_validation.train_test_split(X, y, test_size=chunk_size, random_state=seed)\n",
    "    return [X_train,X_val,y_train, y_val, cols[:num_cols-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformMinMaxTrainData(data):\n",
    "    #Reorder the data to have continuous variables come first\n",
    "    continuous = []\n",
    "    categorical = []\n",
    "    final_columns = []\n",
    "    for col in data.columns.tolist():\n",
    "        if col == 'Cover_Type':\n",
    "            pass\n",
    "        elif data[col].nunique() > 4:\n",
    "            continuous.append(col)\n",
    "        else:\n",
    "            categorical.append(col)\n",
    "    final_columns.extend(continuous)\n",
    "    final_columns.extend(categorical)\n",
    "    final_columns.append('Cover_Type')\n",
    "    data = data[final_columns]\n",
    "    num_row, num_cols = data.shape\n",
    "    cols = data.columns\n",
    "    size = len(continuous) # Number of continuous columns\n",
    "    #Create the data arrays for model building\n",
    "    val_array = data.values\n",
    "    X = val_array[:,0:(num_cols-1)]\n",
    "    y = val_array[:,(num_cols-1)]\n",
    "    X_train, X_val, y_train, y_val = cross_validation.train_test_split(X, y, test_size=chunk_size, random_state=seed)\n",
    "    # MinMax Scale the data\n",
    "    X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "    X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "    # Recombine data\n",
    "    X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "    X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "    return [X_con,X_val_con,y_train, y_val,cols[:num_cols-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformTestData(data):\n",
    "    #Reorder the data to have continuous variables come first\n",
    "    continuous = []\n",
    "    categorical = []\n",
    "    final_columns = []\n",
    "    for col in data.columns.tolist():\n",
    "        if data[col].nunique() > 4:\n",
    "            continuous.append(col)\n",
    "        else:\n",
    "            categorical.append(col)\n",
    "    final_columns.extend(continuous)\n",
    "    final_columns.extend(categorical)\n",
    "    data = data[final_columns]\n",
    "    num_row, num_cols = data.shape\n",
    "    cols = data.columns\n",
    "    size = len(continuous) # Number of continuous columns\n",
    "    #Create the data arrays for model building\n",
    "    X = data.values\n",
    "    return [X, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformMinMaxTestData(data):\n",
    "    #Reorder the data to have continuous variables come first\n",
    "    continuous = []\n",
    "    categorical = []\n",
    "    final_columns = []\n",
    "    for col in data.columns.tolist():\n",
    "        if data[col].nunique() > 4:\n",
    "            continuous.append(col)\n",
    "        else:\n",
    "            categorical.append(col)\n",
    "    final_columns.extend(continuous)\n",
    "    final_columns.extend(categorical)\n",
    "    data = data[final_columns]\n",
    "    num_row, num_cols = data.shape\n",
    "    cols = data.columns\n",
    "    size = len(continuous) # Number of continuous columns\n",
    "    #Create the data arrays for model building\n",
    "    X = data.values\n",
    "    # MinMax Scale the data\n",
    "    X_temp = MinMaxScaler().fit_transform(X[:,0:size])\n",
    "    # Recombine data\n",
    "    X_con = np.concatenate((X_temp,X[:,size:]),axis=1)\n",
    "    return [X_con, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        'data name':'original',\n",
    "        'train':transformTrainData(forest_train),\n",
    "        'test':transformTestData(forest_test)\n",
    "    },\n",
    "    {\n",
    "        'data name':'original scaled',\n",
    "        'train':transformMinMaxTrainData(forest_train),\n",
    "        'test':transformMinMaxTestData(forest_test)\n",
    "    },\n",
    "    {\n",
    "        'data name':'engineered',\n",
    "        'train':transformTrainData(forest_train_eng),\n",
    "        'test':transformTestData(forest_test_eng)\n",
    "    },\n",
    "    {\n",
    "        'data name':'engineered scaled',\n",
    "        'train':transformMinMaxTrainData(forest_train_eng),\n",
    "        'test':transformMinMaxTestData(forest_test_eng)\n",
    "    },\n",
    "    {\n",
    "        'data name':'top 100',\n",
    "        'train':transformTrainData(forest_train_100),\n",
    "        'test':transformTestData(forest_test_100)\n",
    "    },\n",
    "    {\n",
    "        'data name':'top 100 scaled',\n",
    "        'train':transformMinMaxTrainData(forest_train_100),\n",
    "        'test':transformMinMaxTestData(forest_test_100)\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create classifiers and Grid Search\n",
    "- Logistic Regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    {\n",
    "        'name':'Logistic Regression',\n",
    "        'model':LogisticRegression(random_state=seed)\n",
    "    },\n",
    "    {\n",
    "        'name':'SVM',\n",
    "        'model':LinearSVC(random_state=seed)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "original\n",
      "0.643518518519\n",
      "6392\n",
      "                Id\n",
      "Cover_Type        \n",
      "1            39809\n",
      "2            65833\n",
      "3            17175\n",
      "5               99\n",
      "6           441779\n",
      "7             1197\n",
      "original scaled\n",
      "0.647486772487\n",
      "4784\n",
      "                Id\n",
      "Cover_Type        \n",
      "1           170893\n",
      "2           191787\n",
      "3            33089\n",
      "4            11698\n",
      "5            94380\n",
      "6            21497\n",
      "7            42548\n",
      "engineered\n",
      "0.650793650794\n",
      "5578\n",
      "                Id\n",
      "Cover_Type        \n",
      "1.0         475351\n",
      "2.0          37562\n",
      "3.0          49555\n",
      "4.0            147\n",
      "5.0            583\n",
      "6.0           2058\n",
      "7.0            636\n",
      "engineered scaled\n",
      "0.668650793651\n",
      "4864\n",
      "                Id\n",
      "Cover_Type        \n",
      "1.0         174647\n",
      "2.0         178264\n",
      "3.0          26287\n",
      "4.0           7114\n",
      "5.0         100881\n",
      "6.0          29363\n",
      "7.0          49336\n",
      "top 100\n",
      "0.665343915344\n",
      "6693\n",
      "                Id\n",
      "Cover_Type        \n",
      "1.0         486873\n",
      "2.0          26545\n",
      "3.0          49522\n",
      "4.0           1276\n",
      "5.0           1165\n",
      "6.0            357\n",
      "7.0            154\n",
      "top 100 scaled\n",
      "0.687830687831\n",
      "5779\n",
      "                Id\n",
      "Cover_Type        \n",
      "1.0         165559\n",
      "2.0         185984\n",
      "3.0          31731\n",
      "4.0           6291\n",
      "5.0          83732\n",
      "6.0          27952\n",
      "7.0          64643\n",
      "\n",
      "SVM\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "     verbose=0)\n",
      "original\n",
      "0.326058201058\n",
      "5623\n",
      "                Id\n",
      "Cover_Type        \n",
      "6           565892\n",
      "original scaled\n",
      "0.640211640212\n",
      "4873\n",
      "                Id\n",
      "Cover_Type        \n",
      "1           175891\n",
      "2           175916\n",
      "3            44156\n",
      "4            11582\n",
      "5           101077\n",
      "6             9884\n",
      "7            47386\n",
      "engineered\n",
      "0.368386243386\n",
      "6842\n",
      "                Id\n",
      "Cover_Type        \n",
      "3.0         560144\n",
      "4.0           1944\n",
      "6.0           3804\n",
      "engineered scaled\n",
      "0.667989417989\n",
      "4617\n",
      "                Id\n",
      "Cover_Type        \n",
      "1.0         178247\n",
      "2.0         166718\n",
      "3.0          36012\n",
      "4.0           7630\n",
      "5.0         114602\n",
      "6.0          15958\n",
      "7.0          46725\n",
      "top 100\n",
      "0.292989417989\n",
      "5477\n",
      "                Id\n",
      "Cover_Type        \n",
      "1.0          21179\n",
      "3.0         176645\n",
      "5.0          81700\n",
      "6.0         286155\n",
      "7.0            213\n",
      "top 100 scaled\n",
      "0.705687830688\n",
      "4792\n",
      "                Id\n",
      "Cover_Type        \n",
      "1.0         173130\n",
      "2.0         181662\n",
      "3.0          32808\n",
      "4.0           6783\n",
      "5.0          81838\n",
      "6.0          29156\n",
      "7.0          60515\n"
     ]
    }
   ],
   "source": [
    "# Determine feature importance for each model and transformation combination\n",
    "with open('model_testing.txt', 'w+') as file:\n",
    "    for model in features:\n",
    "        print ('')\n",
    "        print (model['name'])\n",
    "        print (model['model'])\n",
    "        for d in datasets:\n",
    "            print (d['data name'])\n",
    "            X_train,X_val,y_train, y_val, cols = d['train']\n",
    "            X_test, cols_test = d['test']\n",
    "            model['model'].fit(X_train, y_train)\n",
    "            print (model['model'].score(X_val, y_val))\n",
    "            print (confusion_matrix_scoring(model['model'].predict(X_val), y_val))\n",
    "            output_df['Cover_Type'] = model['model'].predict(X_test)\n",
    "            print (output_df.groupby(['Cover_Type']).count())\n",
    "            output_df.to_csv('%s_%s_prediction.csv'%(model['name'], d['data name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
