{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Study of Feature Importance in the Forest Cover Type Prediction Dataset\n",
    "\n",
    "\n",
    "Data source: https://www.kaggle.com/c/forest-cover-type-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = pd.read_csv(\"../data/train.csv\", index_col=0) \n",
    "forest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelSoilType(row):\n",
    "    \"\"\"\n",
    "    Label soil types\n",
    "    \"\"\"\n",
    "    for i in range(len(row)):\n",
    "        if row[i] == 1:\n",
    "            return 'Soil_Type'+str(i)\n",
    "        \n",
    "def azimuth_to_abs(x):\n",
    "    \"\"\"\n",
    "    Only care about the absolute angle from 0 w/o respect to direction\n",
    "    \"\"\"\n",
    "    if x>180:\n",
    "        return 360-x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "#### Much of the inspiration of these can be found in the feature exploration notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Soil Type Buckets\n",
    "soil_types = pd.read_csv('soil_types.csv').set_index('Soil Type')\n",
    "forest['Soil Type'] = forest[['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
    "       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7',\n",
    "       'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n",
    "       'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15',\n",
    "       'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19',\n",
    "       'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23',\n",
    "       'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27',\n",
    "       'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31',\n",
    "       'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35',\n",
    "       'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39',\n",
    "       'Soil_Type40']].apply(lambda row: labelSoilType(row), axis=1)\n",
    "forest = pd.merge(forest, soil_types, how='left', left_on='Soil Type', right_index=True)\n",
    "del forest['Soil Type'] # Delete string column\n",
    "\n",
    "# Create feature to that transforms azimuth to its absolute value\n",
    "forest['Aspect2'] = forest.Aspect.map(azimuth_to_abs)\n",
    "forest['Aspect2'].astype(int)\n",
    "\n",
    "# Create feature that determines if the patch is above sea level\n",
    "forest['Above_Sealevel'] = (forest.Vertical_Distance_To_Hydrology>0).astype(int)\n",
    "\n",
    "# Bin the Elevation Feature: check the feature exploration notebook for motivation\n",
    "bins = [0, 2600, 3100, 8000]\n",
    "group_names = [1, 2, 3]\n",
    "forest['Elevation_Bucket'] = pd.cut(forest['Elevation'], bins, labels=group_names)\n",
    "forest['Elevation_0_2600'] = np.where(forest['Elevation_Bucket']== 1, 1, 0)\n",
    "forest['Elevation_2600_3100'] = np.where(forest['Elevation_Bucket']== 2, 1, 0)\n",
    "forest['Elevation_3100_8000'] = np.where(forest['Elevation_Bucket']== 3, 1, 0)\n",
    "forest['Elevation_0_2600'].astype(int)\n",
    "forest['Elevation_2600_3100'].astype(int)\n",
    "forest['Elevation_3100_8000'].astype(int)\n",
    "del forest['Elevation_Bucket']\n",
    "\n",
    "# Create a feature for no hillshade at 3pm\n",
    "forest['3PM_0_Hillshade'] = (forest.Hillshade_3pm == 0).astype(int)\n",
    "\n",
    "#Direct distance to hydrology\n",
    "forest['Direct_Distance_To_Hydrology'] = np.sqrt((forest.Vertical_Distance_To_Hydrology**2) + \\\n",
    "    (forest.Horizontal_Distance_To_Hydrology**2)).astype(float).round(2)\n",
    "\n",
    "\n",
    "soil_types= ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
    "       'Soil_Type4', 'Soil_Type5', 'Soil_Type6',\n",
    "       'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n",
    "       'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "       'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19',\n",
    "       'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23',\n",
    "       'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27',\n",
    "       'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31',\n",
    "       'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35',\n",
    "       'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39',\n",
    "       'Soil_Type40', 'Cover_Type']\n",
    "\n",
    "column_list = forest.columns.tolist()\n",
    "column_list = [c for c in column_list if c[:9] != 'Soil_Type']\n",
    "column_list.insert(10, 'Direct_Distance_To_Hydrology')\n",
    "column_list.insert(11, 'Elevation_0_2600')\n",
    "column_list.insert(12, 'Elevation_2600_3100')\n",
    "column_list.insert(13, 'Elevation_3100_8000')\n",
    "column_list.insert(14, 'Aspect2')\n",
    "column_list.insert(15, 'Above_Sealevel')\n",
    "column_list.insert(16, '3PM_0_Hillshade')\n",
    "column_list.extend(soil_types)\n",
    "columns = []\n",
    "for col in column_list:\n",
    "    if col not in columns:\n",
    "        if col != 'Cover_Type':\n",
    "            columns.append(col)\n",
    "columns.append('Cover_Type')\n",
    "        \n",
    "\n",
    "forest = forest[columns]\n",
    "forest.fillna(0,inplace=True) # Replace nans with 0 for our soil type bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Base Features with no Modeling Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_remove = [] # features to drop\n",
    "for c in forest.columns.tolist():\n",
    "    if forest[c].std() == 0:\n",
    "        to_remove.append(c)\n",
    "forest = forest.drop(to_remove, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Feature Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(forest.shape[1]-1):\n",
    "    for j in range(54):\n",
    "        if i != j:\n",
    "            forest[forest.columns.tolist()[i]+\"_\"+forest.columns.tolist()[j]] = forest[forest.columns.tolist()[i]]*forest[forest.columns.tolist()[j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Columns That Have No Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_remove = [] # features to drop\n",
    "for c in forest.columns.tolist():\n",
    "    if forest[c].std() == 0:\n",
    "        to_remove.append(c)\n",
    "forest = forest.drop(to_remove, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transform the continuous features\n",
    "###### We will try Normalization, Standardized Scaling, and MinMax Scaling\n",
    "###### Note: there is no need to impute any data points as this is a pretty clean data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists and variables\n",
    "chunk_size = 0.1 #Validation chunk size\n",
    "seed = 0 # Use the same random seed to ensure consistent validation chunk usage\n",
    "\n",
    "ranks1 = [] #array of importance rank of all features\n",
    "X_all1 = [] # all features\n",
    "X_all_add1 = [] # Additionally we will make a list of subsets\n",
    "rem1 = [] # columns to be dropped\n",
    "i_rem1 = [] # indexes of columns to be dropped\n",
    "trans_list1 = [] # Transformations\n",
    "comb1 = [] # combinations\n",
    "comb1.append(\"All+1.0\")\n",
    "\n",
    "ranks2 = [] #array of importance rank of all features\n",
    "X_all2 = [] # all features\n",
    "X_all_add2 = [] # Additionally we will make a list of subsets\n",
    "rem2 = [] # columns to be dropped\n",
    "i_rem2 = [] # indexes of columns to be dropped\n",
    "trans_list2 = [] # Transformations\n",
    "comb2 = [] # combinations\n",
    "comb2.append(\"All+1.0\")\n",
    "\n",
    "ranks3 = [] #array of importance rank of all features\n",
    "X_all3 = [] # all features\n",
    "X_all_add3 = [] # Additionally we will make a list of subsets\n",
    "rem3 = [] # columns to be dropped\n",
    "i_rem3 = [] # indexes of columns to be dropped\n",
    "trans_list3 = [] # Transformations\n",
    "comb3 = [] # combinations\n",
    "comb3.append(\"All+1.0\")\n",
    "\n",
    "ranks4 = [] #array of importance rank of all features\n",
    "X_all4 = [] # all features\n",
    "X_all_add4 = [] # Additionally we will make a list of subsets\n",
    "rem4 = [] # columns to be dropped\n",
    "i_rem4 = [] # indexes of columns to be dropped\n",
    "trans_list4 = [] # Transformations\n",
    "comb4 = [] # combinations\n",
    "comb4.append(\"All+1.0\")\n",
    "\n",
    "ratio_list = [0.75,0.50,0.25] #Select top 75%, 50%, 25% of features\n",
    "features = [] # feature selection models\n",
    "model_features = [] # names of feature selection models\n",
    "\n",
    "# reorder the data to have continuous variables come first\n",
    "continuous = [] # continuous variables\n",
    "categorical = [] # categorical variables\n",
    "final_columns = [] # final columns list\n",
    "for col in forest.columns.tolist():\n",
    "    if col in to_remove:\n",
    "        pass\n",
    "    elif col == 'Cover_Type':\n",
    "        pass\n",
    "    elif forest[col].nunique() > 4:\n",
    "        continuous.append(col)\n",
    "    else:\n",
    "        categorical.append(col)\n",
    "\n",
    "final_columns.extend(continuous)\n",
    "final_columns.extend(categorical)\n",
    "final_columns.append('Cover_Type')\n",
    "forest = forest[final_columns]\n",
    "num_rows, num_cols = forest.shape\n",
    "cols = forest.columns\n",
    "size = len(continuous) # Number of continuous columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i_cols = []\n",
    "for i in range(0,num_cols-1):\n",
    "    i_cols.append(i)\n",
    "\n",
    "i_cols1=i_cols[0:int((num_cols-1)/4)]\n",
    "i_cols2=i_cols[int((num_cols-1)/4):int((num_cols-1)/2)]\n",
    "i_cols3=i_cols[int((num_cols-1)/2):int(3*(num_cols-1)/4)]\n",
    "i_cols4=i_cols[int(3*(num_cols-1)/4):(num_cols-1)]\n",
    "\n",
    "cols1 = forest.columns[0:int((num_cols-1)/4)]\n",
    "cols2 = forest.columns[int((num_cols-1)/4):int((num_cols-1)/2)]\n",
    "cols3 = forest.columns[int((num_cols-1)/2):int(3*(num_cols-1)/4)]\n",
    "cols4 = forest.columns[int(3*(num_cols-1)/4):(num_cols-1)]\n",
    "\n",
    "# Create the data arrays for model building\n",
    "val_array = forest.values\n",
    "X1 = val_array[:,0:int((num_cols-1)/4)]\n",
    "y1 = val_array[:,(num_cols-1)]\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(X1, y1, test_size=chunk_size, random_state=seed)\n",
    "X_all1.append(['Orig','X1', X_train1,X_val1,1.0,cols1,rem1,ranks1,i_cols1,i_rem1])\n",
    "\n",
    "X2 = val_array[:,int((num_cols-1)/4):int((num_cols-1)/2)]\n",
    "y2 = val_array[:,(num_cols-1)]\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(X2, y2, test_size=chunk_size, random_state=seed)\n",
    "X_all2.append(['Orig','X2', X_train2,X_val2,1.0,cols2,rem2,ranks2,i_cols2,i_rem2])\n",
    "\n",
    "X3 = val_array[:,int((num_cols-1)/2):int(3*(num_cols-1)/4)]\n",
    "y3 = val_array[:,(num_cols-1)]\n",
    "X_train3, X_val3, y_train3, y_val3 = train_test_split(X3, y3, test_size=chunk_size, random_state=seed)\n",
    "X_all3.append(['Orig','X3', X_train3,X_val3,1.0,cols3,rem3,ranks3,i_cols3,i_rem3])\n",
    "\n",
    "X4 = val_array[:,int(3*(num_cols-1)/4):(num_cols-1)]\n",
    "y4 = val_array[:,(num_cols-1)]\n",
    "X_train4, X_val4, y_train4, y_val4 = train_test_split(X4, y4, test_size=chunk_size, random_state=seed)\n",
    "X_all4.append(['Orig','X4', X_train4,X_val4,1.0,cols4,rem4,ranks4,i_cols4,i_rem4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_temp1 = StandardScaler().fit_transform(X_train1)\n",
    "X_val_temp1 = StandardScaler().fit_transform(X_val1)\n",
    "\n",
    "X_all1.append(['StdSca','All', X_temp1,X_val_temp1,1.0,cols1,rem1,ranks1,i_cols1,i_rem1])\n",
    "\n",
    "\n",
    "# MinMax Scale the data\n",
    "\n",
    "X_temp1 = MinMaxScaler().fit_transform(X_train1)\n",
    "X_val_temp1 = MinMaxScaler().fit_transform(X_val1)\n",
    "\n",
    "X_all1.append(['MinMax', 'All', X_temp1,X_val_temp1,1.0,cols1,rem1,ranks1,i_cols1,i_rem1])\n",
    "\n",
    "\n",
    "#Normalize the data\n",
    "\n",
    "X_temp1 = Normalizer().fit_transform(X_train1)\n",
    "X_val_temp1 = Normalizer().fit_transform(X_val1)\n",
    "\n",
    "X_all1.append(['Norm', 'All', X_temp1,X_val_temp1,1.0,cols1,rem1,ranks1,i_cols1,i_rem1])\n",
    "\n",
    "\n",
    "# Add transformation to the list\n",
    "for trans,name,X1,X_val1,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all1:\n",
    "    trans_list1.append(trans)\n",
    "trans_list1\n",
    "\n",
    "# Standardize the data\n",
    "\n",
    "X_temp2 = StandardScaler().fit_transform(X_train2)\n",
    "X_val_temp2 = StandardScaler().fit_transform(X_val2)\n",
    "\n",
    "X_all2.append(['StdSca','All', X_temp2,X_val_temp2,1.0,cols2,rem2,ranks2,i_cols2,i_rem2])\n",
    "\n",
    "\n",
    "# MinMax Scale the data\n",
    "\n",
    "X_temp2 = MinMaxScaler().fit_transform(X_train2)\n",
    "X_val_temp2 = MinMaxScaler().fit_transform(X_val2)\n",
    "\n",
    "X_all2.append(['MinMax', 'All', X_temp2,X_val_temp2,1.0,cols2,rem2,ranks2,i_cols2,i_rem2])\n",
    "\n",
    "#Normalize the data\n",
    "\n",
    "X_temp2 = Normalizer().fit_transform(X_train2)\n",
    "X_val_temp2 = Normalizer().fit_transform(X_val2)\n",
    "\n",
    "X_all2.append(['Norm', 'All', X_temp2,X_val_temp2,1.0,cols2,rem2,ranks2,i_cols2,i_rem2])\n",
    "\n",
    "\n",
    "# Add transformation to the list\n",
    "for trans,name,X2,X_val2,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all2:\n",
    "    trans_list2.append(trans)\n",
    "trans_list2\n",
    "\n",
    "# Standardize the data\n",
    "\n",
    "X_temp3 = StandardScaler().fit_transform(X_train3[:,0:size-int((num_cols-1)/2)])\n",
    "X_val_temp3 = StandardScaler().fit_transform(X_val3[:,0:size-int((num_cols-1)/2)])\n",
    "\n",
    "# Recombine data\n",
    "X_con3 = np.concatenate((X_temp3,X_train3[:,size-int((num_cols-1)/2):]),axis=1)\n",
    "X_val_con3 = np.concatenate((X_val_temp3,X_val3[:,size-int((num_cols-1)/2):]),axis=1)\n",
    "\n",
    "X_all3.append(['StdSca','All', X_con3,X_val_con3,1.0,cols3,rem3,ranks3,i_cols3,i_rem3])\n",
    "\n",
    "\n",
    "\n",
    "# MinMax Scale the data\n",
    "\n",
    "X_temp3 = MinMaxScaler().fit_transform(X_train3[:,0:size-int((num_cols-1)/2)])\n",
    "X_val_temp3 = MinMaxScaler().fit_transform(X_val3[:,0:size-int((num_cols-1)/2)])\n",
    "\n",
    "# Recombine data\n",
    "X_con3 = np.concatenate((X_temp3,X_train3[:,size-int((num_cols-1)/2):]),axis=1)\n",
    "X_val_con3 = np.concatenate((X_val_temp3,X_val3[:,size-int((num_cols-1)/2):]),axis=1)\n",
    "\n",
    "X_all3.append(['MinMax', 'All', X_con3,X_val_con3,1.0,cols3,rem3,ranks3,i_cols3,i_rem3])\n",
    "\n",
    "\n",
    "#Normalize the data\n",
    "\n",
    "X_temp3 = Normalizer().fit_transform(X_train3[:,0:size-int((num_cols-1)/2)])\n",
    "X_val_temp3 = Normalizer().fit_transform(X_val3[:,0:size-int((num_cols-1)/2)])\n",
    "\n",
    "# Recombine data\n",
    "X_con3 = np.concatenate((X_temp3,X_train3[:,size-int((num_cols-1)/2):]),axis=1)\n",
    "X_val_con3 = np.concatenate((X_val_temp3,X_val3[:,size-int((num_cols-1)/2):]),axis=1)\n",
    "\n",
    "X_all3.append(['Norm', 'All', X_con3,X_val_con3,1.0,cols3,rem3,ranks3,i_cols3,i_rem3])\n",
    "\n",
    "\n",
    "# Add transformation to the list\n",
    "for trans,name,X3,X_val3,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all3:\n",
    "    trans_list3.append(trans)\n",
    "trans_list3\n",
    "\n",
    "# Standardize the data\n",
    "X_all4.append(['StdSca','All', X_train4,X_val4,1.0,cols4,rem4,ranks4,i_cols4,i_rem4])\n",
    "\n",
    "# MinMax Scale the data\n",
    "X_all4.append(['MinMax', 'All', X_train4,X_val4,1.0,cols4,rem4,ranks4,i_cols4,i_rem4])\n",
    "\n",
    "#Normalize the data\n",
    "X_all4.append(['Norm', 'All', X_train4,X_val4,1.0,cols4,rem4,ranks4,i_cols4,i_rem4])\n",
    "\n",
    "# Add transformation to the list\n",
    "for trans,name,X4,X_val4,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all4:\n",
    "    trans_list4.append(trans)\n",
    "trans_list4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the classifiers for measuring importance\n",
    "- Extra Trees Classifier\n",
    "- Gradient Boosting Classifier\n",
    "- Random Forest Classifier\n",
    "- XGBoost Classifier\n",
    "- Random Feature Elimination Classifier\n",
    "- Select Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add Extra Trees Classifier\n",
    "n = 'Extra Trees Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb1.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,ExtraTreesClassifier(n_estimators=len(cols1),max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add Random Forest Classifiers\n",
    "n = 'Random Forest Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb1.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,RandomForestClassifier(n_estimators=len(cols1),max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add XGBoost Classifier\n",
    "n = 'XGBoost Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb1.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,XGBClassifier(n_estimators=len(cols1),seed=seed)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run All the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine feature importance for each model and transformation combination\n",
    "\n",
    "for trans, s, X1, X_val1, d, cols1, rem1, ra1, i_cols1, i_rem1 in X_all1:\n",
    "    for name, v, model in features:\n",
    "        print (\"Training \", name, \"with\", v*100, \"% features and a\", trans, \"transformation\")\n",
    "        # Train the model against y\n",
    "        model.fit(X1,y_train1)\n",
    "\n",
    "        # Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        if name == \"Random Feature Elimination\":\n",
    "            for i, pred in enumerate(list(model.ranking_)):\n",
    "                joined.append([i,cols1[i],pred])\n",
    "        elif name == \"Select Percentile\":\n",
    "            for i, pred in enumerate(list(model.scores_)):\n",
    "                joined.append([i,cols1[i],pred])\n",
    "        else:\n",
    "            for i, pred in enumerate(list(model.feature_importances_)):\n",
    "                joined.append([i,cols1[i],pred])\n",
    "\n",
    "        cols_list = [] # List of names of columns selected\n",
    "        i_cols_list = [] # Indexes of columns selected\n",
    "        rank_list =[] # Ranking of all the columns\n",
    "        rem_list = [] # List of columns not selected\n",
    "        i_rem_list = [] # Indexes of columns not selected\n",
    "\n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2]) # Sort in descending order\n",
    "        rem_start = int(v*(len(cols1))) # Starting point of the columns to be dropped\n",
    "\n",
    "        # Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            rank_list.append([i,j])\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)\n",
    "\n",
    "        # Sort the rank_list and store only the ranks. Drop the index\n",
    "        # Append model name, array, columns selected and columns to be removed to the additional list\n",
    "        X_all_add1.append([trans,name,X1,X_val1,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])\n",
    "\n",
    "\n",
    "rank_df = pd.DataFrame(data=[x[7] for x in X_all_add1],columns=cols1)\n",
    "med = rank_df.median()\n",
    "med.sort()\n",
    "top50_set1=med[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio_list = [0.75,0.50,0.25] #Select top 75%, 50%, 25% of features\n",
    "features = [] # feature selection models\n",
    "model_features = [] # names of feature selection models\n",
    "\n",
    "# Add Extra Trees Classifier\n",
    "n = 'Extra Trees Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb2.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,ExtraTreesClassifier(n_estimators=len(cols2),max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add Random Forest Classifiers\n",
    "n = 'Random Forest Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb2.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,RandomForestClassifier(n_estimators=len(cols2),max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add XGBoost Classifier\n",
    "n = 'XGBoost Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb2.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,XGBClassifier(n_estimators=len(cols2),seed=seed)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine feature importance for each model and transformation combination\n",
    "\n",
    "for trans, s, X2, X_val2, d, cols2, rem2, ra2, i_cols2, i_rem2 in X_all2:\n",
    "    for name, v, model in features:\n",
    "        print (\"Training \", name, \"with\", v*100, \"% features and a\", trans, \"transformation\")\n",
    "        # Train the model against y\n",
    "        model.fit(X2,y_train2)\n",
    "\n",
    "        # Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        if name == \"Random Feature Elimination\":\n",
    "            for i, pred in enumerate(list(model.ranking_)):\n",
    "                joined.append([i,cols2[i],pred])\n",
    "        elif name == \"Select Percentile\":\n",
    "            for i, pred in enumerate(list(model.scores_)):\n",
    "                joined.append([i,cols2[i],pred])\n",
    "        else:\n",
    "            for i, pred in enumerate(list(model.feature_importances_)):\n",
    "                joined.append([i,cols2[i],pred])\n",
    "\n",
    "        cols_list = [] # List of names of columns selected\n",
    "        i_cols_list = [] # Indexes of columns selected\n",
    "        rank_list =[] # Ranking of all the columns\n",
    "        rem_list = [] # List of columns not selected\n",
    "        i_rem_list = [] # Indexes of columns not selected\n",
    "\n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2]) # Sort in descending order\n",
    "        rem_start = int(v*(len(cols2))) # Starting point of the columns to be dropped\n",
    "\n",
    "        # Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            rank_list.append([i,j])\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)\n",
    "\n",
    "        # Sort the rank_list and store only the ranks. Drop the index\n",
    "        # Append model name, array, columns selected and columns to be removed to the additional list\n",
    "        X_all_add2.append([trans,name,X2,X_val2,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])\n",
    "\n",
    "\n",
    "rank_df = pd.DataFrame(data=[x[7] for x in X_all_add2],columns=cols2)\n",
    "med = rank_df.median()\n",
    "med.sort()\n",
    "top50_set2=med[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Extra Trees Classifier\n",
    "ratio_list = [0.75,0.50,0.25] #Select top 75%, 50%, 25% of features\n",
    "features = [] # feature selection models\n",
    "model_features = [] # names of feature selection models\n",
    "# Add Extra Trees Classifier\n",
    "n = 'Extra Trees Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb3.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,ExtraTreesClassifier(n_estimators=len(cols3),max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add Random Forest Classifiers\n",
    "n = 'Random Forest Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb3.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,RandomForestClassifier(n_estimators=len(cols3),max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add XGBoost Classifier\n",
    "n = 'XGBoost Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb3.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,XGBClassifier(n_estimators=len(cols3),seed=seed)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine feature importance for each model and transformation combination\n",
    "\n",
    "for trans, s, X3, X_val3, d, cols3, rem3, ra3, i_cols3, i_rem3 in X_all3:\n",
    "    for name, v, model in features:\n",
    "        print (\"Training \", name, \"with\", v*100, \"% features and a\", trans, \"transformation\")\n",
    "        # Train the model against y\n",
    "        model.fit(X3,y_train3)\n",
    "\n",
    "        # Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        if name == \"Random Feature Elimination\":\n",
    "            for i, pred in enumerate(list(model.ranking_)):\n",
    "                joined.append([i,cols3[i],pred])\n",
    "        elif name == \"Select Percentile\":\n",
    "            for i, pred in enumerate(list(model.scores_)):\n",
    "                joined.append([i,cols3[i],pred])\n",
    "        else:\n",
    "            for i, pred in enumerate(list(model.feature_importances_)):\n",
    "                joined.append([i,cols3[i],pred])\n",
    "\n",
    "        cols_list = [] # List of names of columns selected\n",
    "        i_cols_list = [] # Indexes of columns selected\n",
    "        rank_list =[] # Ranking of all the columns\n",
    "        rem_list = [] # List of columns not selected\n",
    "        i_rem_list = [] # Indexes of columns not selected\n",
    "\n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2]) # Sort in descending order\n",
    "        rem_start = int(v*(len(cols3))) # Starting point of the columns to be dropped\n",
    "\n",
    "        # Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            rank_list.append([i,j])\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)\n",
    "\n",
    "        # Sort the rank_list and store only the ranks. Drop the index\n",
    "        # Append model name, array, columns selected and columns to be removed to the additional list\n",
    "        X_all_add3.append([trans,name,X3,X_val3,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])\n",
    "\n",
    "\n",
    "rank_df = pd.DataFrame(data=[x[7] for x in X_all_add3],columns=cols3)\n",
    "med = rank_df.median()\n",
    "med.sort()\n",
    "top50_set3=med[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Extra Trees Classifier\n",
    "ratio_list = [0.75,0.50,0.25] #Select top 75%, 50%, 25% of features\n",
    "features = [] # feature selection models\n",
    "model_features = [] # names of feature selection models\n",
    "# Add Extra Trees Classifier\n",
    "n = 'Extra Trees Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb4.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,ExtraTreesClassifier(n_estimators=len(cols4),max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add Random Forest Classifiers\n",
    "n = 'Random Forest Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb4.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,RandomForestClassifier(n_estimators=len(cols4),max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add XGBoost Classifier\n",
    "n = 'XGBoost Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb4.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,XGBClassifier(n_estimators=len(cols4),seed=seed)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine feature importance for each model and transformation combination\n",
    "\n",
    "for trans, s, X4, X_val4, d, cols4, rem4, ra4, i_cols4, i_rem4 in X_all4:\n",
    "    for name, v, model in features:\n",
    "        print (\"Training \", name, \"with\", v*100, \"% features and a\", trans, \"transformation\")\n",
    "        # Train the model against y\n",
    "        model.fit(X4,y_train4)\n",
    "\n",
    "        # Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        if name == \"Random Feature Elimination\":\n",
    "            for i, pred in enumerate(list(model.ranking_)):\n",
    "                joined.append([i,cols4[i],pred])\n",
    "        elif name == \"Select Percentile\":\n",
    "            for i, pred in enumerate(list(model.scores_)):\n",
    "                joined.append([i,cols4[i],pred])\n",
    "        else:\n",
    "            for i, pred in enumerate(list(model.feature_importances_)):\n",
    "                joined.append([i,cols4[i],pred])\n",
    "\n",
    "        cols_list = [] # List of names of columns selected\n",
    "        i_cols_list = [] # Indexes of columns selected\n",
    "        rank_list =[] # Ranking of all the columns\n",
    "        rem_list = [] # List of columns not selected\n",
    "        i_rem_list = [] # Indexes of columns not selected\n",
    "\n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2]) # Sort in descending order\n",
    "        rem_start = int(v*(len(cols4))) # Starting point of the columns to be dropped\n",
    "\n",
    "        # Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            rank_list.append([i,j])\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)\n",
    "\n",
    "        # Sort the rank_list and store only the ranks. Drop the index\n",
    "        # Append model name, array, columns selected and columns to be removed to the additional list\n",
    "        X_all_add4.append([trans,name,X4,X_val4,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])\n",
    "\n",
    "\n",
    "rank_df = pd.DataFrame(data=[x[7] for x in X_all_add4],columns=cols4)\n",
    "med = rank_df.median()\n",
    "med.sort()\n",
    "top50_set4=med[:50]\n",
    "\n",
    "top50_set1.to_csv(\"top_100_set1.csv\")\n",
    "top50_set2.to_csv(\"top_100_set2.csv\")\n",
    "top50_set3.to_csv(\"top_100_set3.csv\")\n",
    "top50_set4.to_csv(\"top_100_set4.csv\")\n",
    "\n",
    "top200 = top50_set4.tolist()\n",
    "top200.extend(top50_set3.tolist())\n",
    "top200.extend(top50_set2.tolist())\n",
    "top200.extend(top50_set1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize lists and variables\n",
    "chunk_size = 0.1 #Validation chunk size\n",
    "seed = 0 # Use the same random seed to ensure consistent validation chunk usage\n",
    "\n",
    "ranks = [] #array of importance rank of all features\n",
    "X_all = [] # all features\n",
    "X_all_add = [] # Additionally we will make a list of subsets\n",
    "rem = [] # columns to be dropped\n",
    "i_rem = [] # indexes of columns to be dropped\n",
    "trans_list = [] # Transformations\n",
    "comb = [] # combinations\n",
    "comb.append(\"All+1.0\")\n",
    "\n",
    "ratio_list = [0.75,0.50,0.25] #Select top 75%, 50%, 25% of features\n",
    "features = [] # feature selection models\n",
    "model_features = [] # names of feature selection models\n",
    "\n",
    "# reorder the data to have continuous variables come first\n",
    "continuous = [] # continuous variables\n",
    "categorical = [] # categorical variables\n",
    "final_columns = [] # final columns list\n",
    "for col in top200:\n",
    "    if col in to_remove:\n",
    "        pass\n",
    "    elif forest[col].nunique() > 4:\n",
    "        continuous.append(col)\n",
    "    else:\n",
    "        categorical.append(col)\n",
    "\n",
    "final_columns.extend(continuous)\n",
    "final_columns.extend(categorical)\n",
    "final_columns.append('Cover_Type')\n",
    "forest = forest[final_columns]\n",
    "num_rows, num_cols = forest.shape\n",
    "cols = forest.columns\n",
    "size = len(continuous) # Number of continuous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_cols = []\n",
    "for i in range(0,num_cols-1):\n",
    "    i_cols.append(i)\n",
    "\n",
    "# Create the data arrays for model building\n",
    "val_array = forest.values\n",
    "X = val_array[:,0:(num_cols-1)]\n",
    "y = val_array[:,(num_cols-1)]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=chunk_size, random_state=seed)\n",
    "X_all.append(['Orig','All', X_train,X_val,1.0,cols[:num_cols-1],rem,ranks,i_cols,i_rem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "\n",
    "X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n",
    "\n",
    "# Recombine data\n",
    "X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "X_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MinMax Scale the data\n",
    "\n",
    "X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "\n",
    "# Recombine data\n",
    "X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "X_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "\n",
    "X_temp = Normalizer().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n",
    "\n",
    "# Recombine data\n",
    "X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "X_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add transformation to the list\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "    trans_list.append(trans)\n",
    "trans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add Extra Trees Classifier\n",
    "n = 'Extra Trees Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,ExtraTreesClassifier(n_estimators=num_cols-1,max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add Random Forest Classifiers\n",
    "n = 'Random Forest Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,RandomForestClassifier(n_estimators=num_cols-1,max_features=val,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "# Add XGBoost Classifier\n",
    "n = 'XGBoost Classifier'\n",
    "model_features.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    features.append([n,val,XGBClassifier(n_estimators=num_cols-1,seed=seed)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine feature importance for each model and transformation combination\n",
    "for trans, s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n",
    "    for name, v, model in features:\n",
    "        print (\"Training \", name, \"with\", v*100, \"% features and a\", trans, \"transformation\")\n",
    "        # Train the model against y\n",
    "        model.fit(X,y_train)\n",
    "\n",
    "        # Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        if name == \"Random Feature Elimination\":\n",
    "            for i, pred in enumerate(list(model.ranking_)):\n",
    "                joined.append([i,cols[i],pred])\n",
    "        elif name == \"Select Percentile\":\n",
    "            for i, pred in enumerate(list(model.scores_)):\n",
    "                joined.append([i,cols[i],pred])\n",
    "        else:\n",
    "            for i, pred in enumerate(list(model.feature_importances_)):\n",
    "                joined.append([i,cols[i],pred])\n",
    "\n",
    "        cols_list = [] # List of names of columns selected\n",
    "        i_cols_list = [] # Indexes of columns selected\n",
    "        rank_list =[] # Ranking of all the columns\n",
    "        rem_list = [] # List of columns not selected\n",
    "        i_rem_list = [] # Indexes of columns not selected\n",
    "\n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2]) # Sort in descending order\n",
    "        rem_start = int((v*(num_cols-1))) # Starting point of the columns to be dropped\n",
    "\n",
    "        # Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            rank_list.append([i,j])\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)\n",
    "\n",
    "        # Sort the rank_list and store only the ranks. Drop the index\n",
    "        # Append model name, array, columns selected and columns to be removed to the additional list\n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rank_df = pd.DataFrame(data=[x[7] for x in X_all_add],columns=cols[:num_cols-1])\n",
    "med = rank_df.median()\n",
    "med.sort()\n",
    "med[:15].to_csv(\"top_15.csv\")\n",
    "med[:100].to_csv(\"top_100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit_transform(forest.ix[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_series=pd.Series(data=forest.ix[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_series2=pd.Series(data=forest.ix[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_series = test_series.tolist()\n",
    "test_series.extend(test_series2.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in test_series:\n",
    "    print(test_series[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
